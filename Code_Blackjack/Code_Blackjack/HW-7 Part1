import matplotlib.pyplot as plt
import numpy as np
import sys
from collections import defaultdict
from lib.envs.blackjack import BlackjackEnv
from lib import plotting

env = BlackjackEnv()

def mc_prediction(policy, env, num_episodes, discount_factor=1.0):
    """
    Monte Carlo prediction algorithm. Calculates the value function
    for a given policy using sampling.
    
    Args:
        policy: A function that maps an observation to action probabilities.
        env: OpenAI gym environment.
        num_episodes: Number of episodes to sample.
        discount_factor: Gamma discount factor.
    
    Returns:
        A dictionary that maps from state -> value.
        The state is a tuple and the value is a float.
    """

    returns_sum = defaultdict(float)
    returns_count = defaultdict(float)
    V = defaultdict(float)

    for i_episode in range(1, num_episodes + 1):
        if i_episode % 1000 == 0:
            print("\rEpisode {}/{}.".format(i_episode, num_episodes), end="")
            sys.stdout.flush()

        episode = []
        state = env.reset()
        
        for t in range(100):
            action = policy(state)
            next_state, reward, done, _ = env.step(action)
            episode.append((state, action, reward))
            if done:
                break
            state = next_state

        states_in_episode = set(x[0] for x in episode)
        for state in states_in_episode:
            first_occurence_idx = next(i for i, x in enumerate(episode) if x[0] == state)
            G = sum([x[2] * (discount_factor ** i) for i, x in enumerate(episode[first_occurence_idx:])])
            returns_sum[state] += G
            returns_count[state] += 1.0
            V[state] = returns_sum[state] / returns_count[state]

    return V

def plot_value_function(V, title):
    """
    Plot the value function as a surface plot.
    """
    min_x = min(k[0] for k in V.keys())
    max_x = max(k[0] for k in V.keys())
    min_y = min(k[1] for k in V.keys())
    max_y = max(k[1] for k in V.keys())

    x_range = np.arange(min_x, max_x + 1)
    y_range = np.arange(min_y, max_y + 1)
    X, Y = np.meshgrid(x_range, y_range)

    Z_no_usable_ace = np.apply_along_axis(lambda _: V[(_[0], _[1], False)], 2, np.dstack([X, Y]))
    Z_usable_ace = np.apply_along_axis(lambda _: V[(_[0], _[1], True)], 2, np.dstack([X, Y]))

    def plot_surface(X, Y, Z, title):
        fig = plt.figure(figsize=(20, 10))
        ax = fig.add_subplot(111, projection='3d')
        surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1,
                               cmap='viridis', edgecolor='none')
        ax.set_title(title)
        ax.set_xlabel('Player Sum')
        ax.set_ylabel('Dealer Showing')
        ax.set_zlabel('Value')
        ax.view_init(ax.elev, -120)
        fig.colorbar(surf)
        plt.show()

    plot_surface(X, Y, Z_no_usable_ace, "{} (No Usable Ace)".format(title))
    plot_surface(X, Y, Z_usable_ace, "{} (Usable Ace)".format(title))

# Policy 1: Stick if the player's sum is > 18, otherwise always hit.
def sample_policy_1(observation):
    score, _, _ = observation
    return 0 if score > 18 else 1

V_1_10k = mc_prediction(sample_policy_1, env, num_episodes=10000)
plot_value_function(V_1_10k, title="Policy 1 - 10,000 Episodes")

V_1_500k = mc_prediction(sample_policy_1, env, num_episodes=500000)
plot_value_function(V_1_500k, title="Policy 1 - 500,000 Episodes")

# Policy 2: Stick if the player's sum is > 17, otherwise hits with 30% probability.
def sample_policy_2(observation):
    score, _, _ = observation
    if score > 17:
        return 0  # Stick
    else:
        return np.random.choice([0, 1], p=[0.3, 0.7])  # Hit with 30% probability

V_2_10k = mc_prediction(sample_policy_2, env, num_episodes=10000)
plot_value_function(V_2_10k, title="Policy 2 - 10,000 Episodes")

V_2_500k = mc_prediction(sample_policy_2, env, num_episodes=500000)
plot_value_function(V_2_500k, title="Policy 2 - 500,000 Episodes")

# Policy 3: Stick if the player's sum is 20 or 21, otherwise always hit.
def sample_policy_3(observation):
    score, _, _ = observation
    return 0 if score in [20, 21] else 1

V_3_10k = mc_prediction(sample_policy_3, env, num_episodes=10000)
plot_value_function(V_3_10k, title="Policy 3 - 10,000 Episodes")

V_3_500k = mc_prediction(sample_policy_3, env, num_episodes=500000)
plot_value_function(V_3_500k, title="Policy 3 - 500,000 Episodes")
